{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsoq5DURzS97",
        "outputId": "14c7f9ba-9eb4-4754-b08e-2d86040c0f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNYfEqG2iiyV",
        "outputId": "a5480e15-e092-47a4-a7ed-a9e148be76b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.23.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.28.68)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.6.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.1.0)\n",
            "Requirement already satisfied: botocore<1.32.0,>=1.31.68 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (1.31.68)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.68->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkqTCHm8l4Qp",
        "outputId": "b8427c93-fd1c-479b-e1c9-431b6ad93f2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.23.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.28.68)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.1.0)\n",
            "Requirement already satisfied: botocore<1.32.0,>=1.31.68 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.31.68)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-pretrained-bert) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.68->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.68->boto3->pytorch-pretrained-bert) (1.16.0)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M14Ujc-NlTsP",
        "outputId": "055e616f-5498-4f62-c7eb-320a3e2451cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dotmap in /usr/local/lib/python3.10/dist-packages (1.3.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVLzi18Ahalw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "912776cb-c010-4771-8700-deb60dc32477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Adm_ID         Note_ID  \\\n",
              "0  22595853  10000032-RR-15   \n",
              "1  22595853  10000032-RR-15   \n",
              "2  22595853  10000032-RR-15   \n",
              "3  22595853  10000032-RR-16   \n",
              "4  22595853  10000032-RR-16   \n",
              "\n",
              "                                            Input_ID  Label  \\\n",
              "0  101 7749 1024 11290 2030 26033 28522 20791 214...      0   \n",
              "1  101 2854 29454 3370 1012 1996 17324 2094 5761 ...      0   \n",
              "2  101 1024 5107 3550 8810 1997 20118 13320 1998 ...      0   \n",
              "3  101 12407 1024 1035 16731 2615 25022 12171 252...      0   \n",
              "4  101 4663 1012 1037 17463 3217 11788 5397 2051 ...      0   \n",
              "\n",
              "             chartdate            charttime  \n",
              "0  2180-05-06 23:00:00  2180-05-06 23:00:00  \n",
              "1  2180-05-06 23:00:00  2180-05-06 23:00:00  \n",
              "2  2180-05-06 23:00:00  2180-05-06 23:00:00  \n",
              "3  2180-05-07 09:55:00  2180-05-07 09:55:00  \n",
              "4  2180-05-07 09:55:00  2180-05-07 09:55:00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a7087e8-6af8-4468-b863-8e86f04df4cd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adm_ID</th>\n",
              "      <th>Note_ID</th>\n",
              "      <th>Input_ID</th>\n",
              "      <th>Label</th>\n",
              "      <th>chartdate</th>\n",
              "      <th>charttime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22595853</td>\n",
              "      <td>10000032-RR-15</td>\n",
              "      <td>101 7749 1024 11290 2030 26033 28522 20791 214...</td>\n",
              "      <td>0</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22595853</td>\n",
              "      <td>10000032-RR-15</td>\n",
              "      <td>101 2854 29454 3370 1012 1996 17324 2094 5761 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22595853</td>\n",
              "      <td>10000032-RR-15</td>\n",
              "      <td>101 1024 5107 3550 8810 1997 20118 13320 1998 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "      <td>2180-05-06 23:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22595853</td>\n",
              "      <td>10000032-RR-16</td>\n",
              "      <td>101 12407 1024 1035 16731 2615 25022 12171 252...</td>\n",
              "      <td>0</td>\n",
              "      <td>2180-05-07 09:55:00</td>\n",
              "      <td>2180-05-07 09:55:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22595853</td>\n",
              "      <td>10000032-RR-16</td>\n",
              "      <td>101 4663 1012 1037 17463 3217 11788 5397 2051 ...</td>\n",
              "      <td>0</td>\n",
              "      <td>2180-05-07 09:55:00</td>\n",
              "      <td>2180-05-07 09:55:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a7087e8-6af8-4468-b863-8e86f04df4cd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a7087e8-6af8-4468-b863-8e86f04df4cd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a7087e8-6af8-4468-b863-8e86f04df4cd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c2ab493-8aba-42b1-a704-43936c5e2fe4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c2ab493-8aba-42b1-a704-43936c5e2fe4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c2ab493-8aba-42b1-a704-43936c5e2fe4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1keKtE29vTJB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "m4_train = pd.read_csv(\"/content/drive/My Drive/m4_train.csv\")\n",
        "m4_val = pd.read_csv(\"/content/drive/My Drive/m4_val.csv\")\n",
        "m4_test = pd.read_csv(\"/content/drive/My Drive/m4_test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(m4_train['Input_ID'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "-07jUca-BY0Z",
        "outputId": "24dfaf6a-da84-4159-c814-9a9ff6e41259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series</b><br/>def __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>One-dimensional ndarray with axis labels (including time series).\n",
              "\n",
              "Labels need not be unique but must be a hashable type. The object\n",
              "supports both integer- and label-based indexing and provides a host of\n",
              "methods for performing operations involving the index. Statistical\n",
              "methods from ndarray have been overridden to automatically exclude\n",
              "missing data (currently represented as NaN).\n",
              "\n",
              "Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n",
              "associated index values-- they need not be the same length. The result\n",
              "index will be the sorted union of the two indexes.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : array-like, Iterable, dict, or scalar value\n",
              "    Contains data stored in Series. If data is a dict, argument order is\n",
              "    maintained.\n",
              "index : array-like or Index (1d)\n",
              "    Values must be hashable and have the same length as `data`.\n",
              "    Non-unique index values are allowed. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n",
              "    and index is None, then the keys in the data are used as the index. If the\n",
              "    index is not None, the resulting Series is reindexed with the index values.\n",
              "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
              "    Data type for the output Series. If not specified, this will be\n",
              "    inferred from `data`.\n",
              "    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n",
              "name : Hashable, default None\n",
              "    The name to give to the Series.\n",
              "copy : bool, default False\n",
              "    Copy input data. Only affects Series or 1d ndarray input. See examples.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing Series from a dictionary with an Index specified\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "a   1\n",
              "b   2\n",
              "c   3\n",
              "dtype: int64\n",
              "\n",
              "The keys of the dictionary match with the Index values, hence the Index\n",
              "values have no effect.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "x   NaN\n",
              "y   NaN\n",
              "z   NaN\n",
              "dtype: float64\n",
              "\n",
              "Note that the Index is first build with the keys from the dictionary.\n",
              "After this the Series is reindexed with the given Index values, hence we\n",
              "get all NaN as a result.\n",
              "\n",
              "Constructing Series from a list with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = [1, 2]\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "[1, 2]\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `copy` of\n",
              "the original data even though `copy=False`, so\n",
              "the data is unchanged.\n",
              "\n",
              "Constructing Series from a 1d ndarray with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = np.array([1, 2])\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "array([999,   2])\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `view` on\n",
              "the original data, so\n",
              "the data is changed as well.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 245);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# 假设m4_train是你的DataFrame\n",
        "m4_train['Input_ID'] = m4_train['Input_ID'].apply(ast.literal_eval)\n",
        "m4_val['Input_ID'] = m4_val['Input_ID'].apply(ast.literal_eval)\n",
        "m4_test['Input_ID'] = m4_test['Input_ID'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "242hg0T-AA96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kxhI3ZCzY5r"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "input_encoder = LabelEncoder()\n",
        "m4_train['input_id_encoded'] = input_encoder.fit_transform(m4_train['Input_ID'])\n",
        "\n",
        "adm_encoder = LabelEncoder()\n",
        "m4_train['adm_id_encoded'] = adm_encoder.fit_transform(m4_train['Adm_ID'])\n",
        "\n",
        "input_encoder = LabelEncoder()\n",
        "m4_val['input_id_encoded'] = input_encoder.fit_transform(m4_val['Input_ID'])\n",
        "\n",
        "adm_encoder = LabelEncoder()\n",
        "m4_val['adm_id_encoded'] = adm_encoder.fit_transform(m4_val['Adm_ID'])\n",
        "\n",
        "input_encoder = LabelEncoder()\n",
        "m4_test['input_id_encoded'] = input_encoder.fit_transform(m4_test['Input_ID'])\n",
        "\n",
        "adm_encoder = LabelEncoder()\n",
        "m4_test['adm_id_encoded'] = adm_encoder.fit_transform(m4_test['Adm_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(m4_train['Adm_ID'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "1qKZEiL0Dwct",
        "outputId": "de61094a-63d1-4e73-862a-ae7786990f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.series.Series</b><br/>def __init__(data=None, index=None, dtype: Dtype | None=None, name=None, copy: bool | None=None, fastpath: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/series.py</a>One-dimensional ndarray with axis labels (including time series).\n",
              "\n",
              "Labels need not be unique but must be a hashable type. The object\n",
              "supports both integer- and label-based indexing and provides a host of\n",
              "methods for performing operations involving the index. Statistical\n",
              "methods from ndarray have been overridden to automatically exclude\n",
              "missing data (currently represented as NaN).\n",
              "\n",
              "Operations between Series (+, -, /, \\*, \\*\\*) align values based on their\n",
              "associated index values-- they need not be the same length. The result\n",
              "index will be the sorted union of the two indexes.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : array-like, Iterable, dict, or scalar value\n",
              "    Contains data stored in Series. If data is a dict, argument order is\n",
              "    maintained.\n",
              "index : array-like or Index (1d)\n",
              "    Values must be hashable and have the same length as `data`.\n",
              "    Non-unique index values are allowed. Will default to\n",
              "    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n",
              "    and index is None, then the keys in the data are used as the index. If the\n",
              "    index is not None, the resulting Series is reindexed with the index values.\n",
              "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
              "    Data type for the output Series. If not specified, this will be\n",
              "    inferred from `data`.\n",
              "    See the :ref:`user guide &lt;basics.dtypes&gt;` for more usages.\n",
              "name : Hashable, default None\n",
              "    The name to give to the Series.\n",
              "copy : bool, default False\n",
              "    Copy input data. Only affects Series or 1d ndarray input. See examples.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.series&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing Series from a dictionary with an Index specified\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "a   1\n",
              "b   2\n",
              "c   3\n",
              "dtype: int64\n",
              "\n",
              "The keys of the dictionary match with the Index values, hence the Index\n",
              "values have no effect.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3}\n",
              "&gt;&gt;&gt; ser = pd.Series(data=d, index=[&#x27;x&#x27;, &#x27;y&#x27;, &#x27;z&#x27;])\n",
              "&gt;&gt;&gt; ser\n",
              "x   NaN\n",
              "y   NaN\n",
              "z   NaN\n",
              "dtype: float64\n",
              "\n",
              "Note that the Index is first build with the keys from the dictionary.\n",
              "After this the Series is reindexed with the given Index values, hence we\n",
              "get all NaN as a result.\n",
              "\n",
              "Constructing Series from a list with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = [1, 2]\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "[1, 2]\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `copy` of\n",
              "the original data even though `copy=False`, so\n",
              "the data is unchanged.\n",
              "\n",
              "Constructing Series from a 1d ndarray with `copy=False`.\n",
              "\n",
              "&gt;&gt;&gt; r = np.array([1, 2])\n",
              "&gt;&gt;&gt; ser = pd.Series(r, copy=False)\n",
              "&gt;&gt;&gt; ser.iloc[0] = 999\n",
              "&gt;&gt;&gt; r\n",
              "array([999,   2])\n",
              "&gt;&gt;&gt; ser\n",
              "0    999\n",
              "1      2\n",
              "dtype: int64\n",
              "\n",
              "Due to input data type the Series has a `view` on\n",
              "the original data, so\n",
              "the data is changed as well.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 245);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NCngu3bmGIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050ba49a-98e1-4f08-c01b-039ace31a9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 92ms/step - accuracy: 0.9160 - loss: 0.2698 - val_accuracy: 0.8989 - val_loss: 0.2718\n",
            "Epoch 2/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 87ms/step - accuracy: 0.9178 - loss: 0.2178 - val_accuracy: 0.9054 - val_loss: 0.2744\n",
            "Epoch 3/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 88ms/step - accuracy: 0.9239 - loss: 0.1942 - val_accuracy: 0.9028 - val_loss: 0.2909\n",
            "Epoch 4/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 88ms/step - accuracy: 0.9312 - loss: 0.1732 - val_accuracy: 0.8991 - val_loss: 0.3166\n",
            "Epoch 5/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 87ms/step - accuracy: 0.9382 - loss: 0.1540 - val_accuracy: 0.8959 - val_loss: 0.3243\n",
            "Epoch 6/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 87ms/step - accuracy: 0.9489 - loss: 0.1272 - val_accuracy: 0.8949 - val_loss: 0.3764\n",
            "Epoch 7/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 88ms/step - accuracy: 0.9598 - loss: 0.1049 - val_accuracy: 0.8838 - val_loss: 0.4446\n",
            "Epoch 8/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 92ms/step - accuracy: 0.9671 - loss: 0.0852 - val_accuracy: 0.8771 - val_loss: 0.4611\n",
            "Epoch 9/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 88ms/step - accuracy: 0.9725 - loss: 0.0699 - val_accuracy: 0.8689 - val_loss: 0.5421\n",
            "Epoch 10/10\n",
            "\u001b[1m2502/2502\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 88ms/step - accuracy: 0.9789 - loss: 0.0541 - val_accuracy: 0.8767 - val_loss: 0.5984\n",
            "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step\n",
            "Accuracy: 0.9006642512077294\n",
            "AUROC: 0.7187433037025591\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "# 将Input_ID从字符串转换为列表\n",
        "m4_train['Input_ID'] = m4_train['Input_ID'].apply(ast.literal_eval)\n",
        "m4_val['Input_ID'] = m4_val['Input_ID'].apply(ast.literal_eval)\n",
        "m4_test['Input_ID'] = m4_test['Input_ID'].apply(ast.literal_eval)\n",
        "# 提取数据\n",
        "X_train = m4_train['Input_ID'].tolist()\n",
        "y_train = m4_train['Label'].values\n",
        "\n",
        "X_val = m4_val['Input_ID'].tolist()\n",
        "y_val = m4_val['Label'].values\n",
        "\n",
        "X_test = m4_test['Input_ID'].tolist()\n",
        "y_test = m4_test['Label'].values\n",
        "\n",
        "# 填充序列，确保所有序列的长度一致\n",
        "maxlen = max(max([len(seq) for seq in X_train]), max([len(seq) for seq in X_val]), max([len(seq) for seq in X_test]))\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "# 模型定义\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=50000, output_dim=128, input_length=maxlen), # 假设字典大小为50000\n",
        "    LSTM(64, return_sequences=True),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 模型训练\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# 预测\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# 计算accuracy和AUROC\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auroc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'AUROC: {auroc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSCXFmTSwKJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401ef11f-8592-4748-c69c-3a33edd64bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 306, 128)          24723712  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 306, 128)          131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 306, 2)            258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24855554 (94.82 MB)\n",
            "Trainable params: 24855554 (94.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zss6Rs-Em9zc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-base-multilingual': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "CONFIG_NAME = 'bert_config.json'\n",
        "WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BertLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BertLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "class PreTrainedBertModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedBertModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.beta.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            module.gamma.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "\n",
        "        Params:\n",
        "            pretrained_model_name: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-base-multilingual`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name\n",
        "        # redirect to the cache, if necessary\n",
        "            return None\n",
        "        #if resolved_archive_file == archive_file:\n",
        "            #logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        #else:\n",
        "           # logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "             #   archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        #if os.path.isdir(resolved_archive_file):\n",
        "         #   serialization_dir = resolved_archive_file\n",
        "        #else:\n",
        "            # Extract archive to temp dir\n",
        "          #  tempdir = tempfile.mkdtemp()\n",
        "          #  logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "          #      resolved_archive_file, tempdir))\n",
        "          #  with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "          #      archive.extractall(tempdir)\n",
        "          #  serialization_dir = tempdir\n",
        "        # Load config\n",
        "        #config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        #config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        #weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "        #state_dict = torch.load(weights_path, map_location = 'cpu')\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "class BertModel(PreTrainedBertModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block,\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaLNuBQMR1Ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e46d7c5-6439-4640-e94c-bf38f5866b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "539/539 [==============================] - 40s 69ms/step - loss: 0.0221 - accuracy: 0.9959 - val_loss: 0.0107 - val_accuracy: 0.9973\n",
            "Epoch 2/10\n",
            "539/539 [==============================] - 18s 33ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0205 - val_accuracy: 0.9970\n",
            "Epoch 3/10\n",
            "539/539 [==============================] - 15s 27ms/step - loss: 4.7524e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9971\n",
            "Epoch 4/10\n",
            "539/539 [==============================] - 14s 26ms/step - loss: 1.5780e-04 - accuracy: 1.0000 - val_loss: 0.0254 - val_accuracy: 0.9970\n",
            "Epoch 5/10\n",
            "539/539 [==============================] - 13s 25ms/step - loss: 7.7871e-05 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9969\n",
            "Epoch 6/10\n",
            "539/539 [==============================] - 13s 24ms/step - loss: 4.5912e-05 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 0.9967\n",
            "Epoch 7/10\n",
            "539/539 [==============================] - 13s 23ms/step - loss: 2.9397e-05 - accuracy: 1.0000 - val_loss: 0.0307 - val_accuracy: 0.9966\n",
            "Epoch 8/10\n",
            "539/539 [==============================] - 12s 23ms/step - loss: 1.9681e-05 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 0.9966\n",
            "Epoch 9/10\n",
            "539/539 [==============================] - 12s 22ms/step - loss: 1.3603e-05 - accuracy: 1.0000 - val_loss: 0.0329 - val_accuracy: 0.9964\n",
            "Epoch 10/10\n",
            "539/539 [==============================] - 12s 22ms/step - loss: 9.6179e-06 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 0.9964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load validation and test sets\n",
        "val_file_path = \"/content/drive/My Drive/m4_val_chunk.csv\"\n",
        "test_file_path = \"/content/drive/My Drive/m4_test_chunk.csv\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npjmF9rPVmEB",
        "outputId": "be0f89ed-6690-4334-f37f-43185522756e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "674/674 [==============================] - 6s 9ms/step - loss: 0.0063 - accuracy: 0.9993\n",
            "Validation Accuracy: 0.9992876052856445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpBjturnXfIH",
        "outputId": "ef7fa16b-61e2-4b19-e420-99bc2daa4143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "674/674 [==============================] - 4s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDuFvPSXaGgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3m9ok5M8vUSZA2oCQc2lH"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}